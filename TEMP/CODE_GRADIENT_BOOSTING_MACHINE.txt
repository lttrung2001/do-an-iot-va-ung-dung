# # Feature importance by Gradient Boosting Machine
# import lightgbm as lgb
# # Initialize an empty array to hold feature importances
# feature_importances = np.zeros(X_train.shape[1])

# # Create the model with several hyperparameters
# model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')

# # Fit the model twice to avoid overfitting
# for i in range(2):
    
#     # Split into training and validation set
#     train_features, valid_features, train_y, valid_y = train_test_split(X_train, Y_train, test_size = 0.25, random_state = i)
#     print(X_train.shape)
    
#     # Train using early stopping
#     model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], 
#               eval_metric = 'auc', verbose = 200)
    
#     # Record the feature importances
#     feature_importances += model.feature_importances_

# # Make sure to average feature importances! 
# feature_importances = feature_importances / 2
# feature_importances = pd.DataFrame({'feature': list(cols), 'importance': feature_importances}).sort_values('importance', ascending = False)

# print(feature_importances.head())

# # Find the features with zero importance
# zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])
# print('There are %d features with 0.0 importance' % len(zero_features))
# print(feature_importances.tail())

# import matplotlib.pyplot as plt
# threshold = 0.99
# def plot_feature_importances(df, threshold = threshold):
#     """
#     Plots 20 most important features and the cumulative importance of features.
#     Prints the number of features needed to reach threshold cumulative importance.
    
#     Parameters
#     --------
#     df : dataframe
#         Dataframe of feature importances. Columns must be feature and importance
#     threshold : float, default = 0.9
#         Threshold for prining information about cumulative importances
        
#     Return
#     --------
#     df : dataframe
#         Dataframe ordered by feature importances with a normalized column (sums to 1)
#         and a cumulative importance column
    
#     """
    
#     plt.rcParams['font.size'] = 18
    
#     # Sort features according to importance
#     df = df.sort_values('importance', ascending = False).reset_index()
    
#     # Normalize the feature importances to add up to one
#     df['importance_normalized'] = df['importance'] / df['importance'].sum()
#     df['cumulative_importance'] = np.cumsum(df['importance_normalized'])

#     # Make a horizontal bar chart of feature importances
#     plt.figure(figsize = (10, 6))
#     ax = plt.subplot()
    
#     # Need to reverse the index to plot most important on top
#     ax.barh(list(reversed(list(df.index[:20]))), 
#             df['importance_normalized'].head(20), 
#             align = 'center', edgecolor = 'k')
    
#     # Set the yticks and labels
#     ax.set_yticks(list(reversed(list(df.index[:20]))))
#     ax.set_yticklabels(df['feature'].head(20))
    
#     # Plot labeling
#     plt.xlabel('Normalized Importance'); plt.title('Feature Importances')
#     plt.show()
    
#     # Cumulative importance plot
#     plt.figure(figsize = (8, 6))
#     plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')
#     plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); 
#     plt.title('Cumulative Feature Importance')
#     plt.show()
    
#     importance_index = np.min(np.where(df['cumulative_importance'] > threshold))
#     print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))
    
#     return df

# norm_feature_importances = plot_feature_importances(feature_importances)